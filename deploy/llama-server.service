[Unit]
Description=llama.cpp inference server (GPU)
After=network.target

[Service]
Type=simple
User=manu
ExecStart=/usr/local/bin/llama-server \
    -m /opt/llama-models/qwen2.5-1.5b-instruct-q4_k_m.gguf \
    --port 8080 --host 127.0.0.1 \
    -ngl 999 -c 2048 -t 4
Restart=on-failure
RestartSec=5

# Hardening
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=read-only
ReadWritePaths=/opt/llama-models
PrivateTmp=true

[Install]
WantedBy=multi-user.target
