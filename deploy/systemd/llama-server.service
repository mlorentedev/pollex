[Unit]
Description=llama.cpp inference server (GPU)
After=network.target

[Service]
Type=simple
User=manu
Environment=GGML_CUDA_ENABLE_UNIFIED_MEMORY=1
ExecStart=/usr/local/bin/llama-server \
    -m /opt/llama-models/qwen2.5-1.5b-instruct-q4_k_m.gguf \
    --port 8080 --host 127.0.0.1 \
    -ngl 999 -c 1024 -t 4 \
    --batch-size 1024 --ubatch-size 512 \
    --mlock
Restart=on-failure
RestartSec=5
LimitMEMLOCK=infinity

# Hardening
NoNewPrivileges=true
ProtectSystem=strict
ProtectHome=read-only
ReadWritePaths=/opt/llama-models
PrivateTmp=true

[Install]
WantedBy=multi-user.target
